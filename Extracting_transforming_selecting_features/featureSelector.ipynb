{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/06 21:37:10 WARN Utils: Your hostname, iamhimanshu0 resolves to a loopback address: 127.0.1.1; using 192.168.43.239 instead (on interface wlo1)\n",
      "22/04/06 21:37:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/06 21:37:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Feature Selection\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VectorSlicer\n",
    "\n",
    "VectorSlicer is a transformer that takes a feature vector and outputs a new feature vector with a sub-array of the original features. It is useful for extracting features from a vector column.\n",
    "\n",
    "VectorSlicer accepts a vector column with specified indices, then outputs a new vector column whose values are selected via those indices. There are two types of indices,\n",
    "\n",
    "+ Integer indices that represent the indices into the vector, setIndices().\n",
    "\n",
    "+ String indices that represent the names of features into the vector, setNames(). This requires the vector column to have an AttributeGroup since the implementation matches on the name field of an Attribute.\n",
    "\n",
    "Examples\n",
    "\n",
    "Suppose that we have a DataFrame with the column userFeatures:\n",
    "\n",
    "userFeatures\n",
    "------------------\n",
    "[0.0, 10.0, 0.5]\n",
    "\n",
    "userFeatures is a vector column that contains three user features. Assume that the first column of userFeatures are all zeros, so we want to remove it and select only the last two columns. The VectorSlicer selects the last two elements with setIndices(1, 2) then produces a new vector column named features:\n",
    "\n",
    " userFeatures     | features\n",
    "------------------|-----------------------------\n",
    " [0.0, 10.0, 0.5] | [10.0, 0.5]\n",
    "\n",
    "Suppose also that we have potential input attributes for the userFeatures, i.e. [\"f1\", \"f2\", \"f3\"], then we can use setNames(\"f2\", \"f3\") to select them.\n",
    "\n",
    " userFeatures     | features\n",
    "------------------|-----------------------------\n",
    " [0.0, 10.0, 0.5] | [10.0, 0.5]\n",
    " [\"f1\", \"f2\", \"f3\"] | [\"f2\", \"f3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        userFeatures|\n",
      "+--------------------+\n",
      "|(3,[0,1],[-2.0,2.3])|\n",
      "|      [-2.0,2.3,0.0]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(userFeatures=Vectors.sparse(3, {0: -2.0, 1: 2.3})),\n",
    "    Row(userFeatures=Vectors.dense([-2.0, 2.3, 0.0]))])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|        userFeatures|     features|\n",
      "+--------------------+-------------+\n",
      "|(3,[0,1],[-2.0,2.3])|(1,[0],[2.3])|\n",
      "|      [-2.0,2.3,0.0]|        [2.3]|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slicer = VectorSlicer(inputCol=\"userFeatures\", outputCol=\"features\", indices=[1])\n",
    "\n",
    "output = slicer.transform(df)\n",
    "\n",
    "output.select(\"userFeatures\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChiSqSelector\n",
    "\n",
    "ChiSqSelector stands for Chi-Squared feature selection. It operates on labeled data with categorical features.\n",
    "\n",
    "ChiSqSelector uses the Chi-Squared test of independence to decide which features to choose. It supports five selection methods: numTopFeatures, percentile, fpr, fdr, fwe:\n",
    "\n",
    "- numTopFeatures chooses a fixed number of top features according to a chi-squared test. This is akin to yielding the features with the most predictive power.\n",
    "- percentile is similar to numTopFeatures but chooses a fraction of all features instead of a fixed number.\n",
    "- fpr chooses all features whose p-values are below a threshold, thus controlling the false positive rate of selection.\n",
    "- fdr uses the Benjamini-Hochberg procedure to choose all features whose false discovery rate is below a threshold.\n",
    "- fwe chooses all features whose p-values are below a threshold. The threshold is scaled by 1/numFeatures, thus controlling the family-wise error rate of selection.\n",
    "\n",
    "By default, the selection method is numTopFeatures, with the default number of top features set to 50. The user can choose a selection method using setSelectorType.\n",
    "\n",
    "Examples\n",
    "\n",
    "Assume that we have a DataFrame with the columns id, features, and clicked, which is used as our target to be predicted:\n",
    "\n",
    "id | features              | clicked\n",
    "---|-----------------------|---------\n",
    " 7 | [0.0, 0.0, 18.0, 1.0] | 1.0\n",
    " 8 | [0.0, 1.0, 12.0, 0.0] | 0.0\n",
    " 9 | [1.0, 0.0, 15.0, 0.1] | 0.0\n",
    "\n",
    "If we use ChiSqSelector with numTopFeatures = 1, then according to our label clicked the last column in our features is chosen as the most useful feature:\n",
    "\n",
    "id | features              | clicked | selectedFeatures\n",
    "---|-----------------------|---------|------------------\n",
    " 7 | [0.0, 0.0, 18.0, 1.0] | 1.0     | [1.0]\n",
    " 8 | [0.0, 1.0, 12.0, 0.0] | 0.0     | [0.0]\n",
    " 9 | [1.0, 0.0, 15.0, 0.1] | 0.0     | [0.1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-------+\n",
      "| id|          features|clicked|\n",
      "+---+------------------+-------+\n",
      "|  7|[0.0,0.0,18.0,1.0]|    1.0|\n",
      "|  8|[0.0,1.0,12.0,0.0]|    0.0|\n",
      "|  9|[1.0,0.0,15.0,0.1]|    0.0|\n",
      "+---+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
    "    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
    "    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 1 features selected\n",
      "+---+------------------+-------+----------------+\n",
      "| id|          features|clicked|selectedFeatures|\n",
      "+---+------------------+-------+----------------+\n",
      "|  7|[0.0,0.0,18.0,1.0]|    1.0|          [18.0]|\n",
      "|  8|[0.0,1.0,12.0,0.0]|    0.0|          [12.0]|\n",
      "|  9|[1.0,0.0,15.0,0.1]|    0.0|          [15.0]|\n",
      "+---+------------------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=1, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
    "\n",
    "result = selector.fit(df).transform(df)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82dc0c27685d313ea4c02f56aa37741b330462340c1074df74913ea85720511c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
