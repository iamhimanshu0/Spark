{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntroNLP Pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqdWa0KzNYni"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Natural Language Processing with PySpark\n",
        "\n",
        "NLP Tools\n",
        "- Tokenizer\n",
        "- StopWordRemoval\n",
        "- n-grams\n",
        "- TF-IDF\n",
        "- CountVectorizer"
      ],
      "metadata": {
        "id": "dh01ZVusNfSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip >> smsspamcollection.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLE4868kNrRo",
        "outputId": "81300b74-a6f2-4c98-89dd-df6451348f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  198k  100  198k    0     0   441k      0 --:--:-- --:--:-- --:--:--  441k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt install unzip\n",
        "!unzip /content/smsspamcollection.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFsdVxX_N4K0",
        "outputId": "adb7279f-850d-4907-8343-9b9bd300d4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGjkv3_VPV7C",
        "outputId": "86058e7a-d6d9-4a08-d481-7b8fb64ab402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 35 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 40.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=017d1532593466cb1f48bc09c014dd638c5ed34575b94ca7872c874828465087\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "Yiy1nC4kOBWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"NLP Learning\").getOrCreate()"
      ],
      "metadata": {
        "id": "MT3oCA9KOTXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizer"
      ],
      "metadata": {
        "id": "TGMFA6PPP1mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType"
      ],
      "metadata": {
        "id": "IagC1EmEPq2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df = spark.createDataFrame(\n",
        "    [\n",
        "     (0,\"hello i am happy to be learning Apache Spark\"),\n",
        "     (1, \"I enjoy learning about python and javascript progamming\"),\n",
        "     (2, \"i am familiar with Machine Learning applications\"),\n",
        "     (3, \"here, is,a,list,of,words\")\n",
        "    ],\n",
        "    ['id','sentences']\n",
        ")\n",
        "\n",
        "sent_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDaNcXvNQPIP",
        "outputId": "f0f0a0d6-e9ed-4f26-8fc5-31445707cdd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------------------------------------+\n",
            "|id |sentences                                              |\n",
            "+---+-------------------------------------------------------+\n",
            "|0  |hello i am happy to be learning Apache Spark           |\n",
            "|1  |I enjoy learning about python and javascript progamming|\n",
            "|2  |i am familiar with Machine Learning applications       |\n",
            "|3  |here, is,a,list,of,words                               |\n",
            "+---+-------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(inputCol='sentences', outputCol='tokenOutput')\n",
        "regexTokenizer = RegexTokenizer(inputCol='sentences', outputCol='regxOutput',pattern=\"\\\\W\")\n",
        "\n",
        "# word count for each sentences\n",
        "countTokens = udf(lambda w:len(w), IntegerType())\n"
      ],
      "metadata": {
        "id": "1-scKI1gQN0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer.transform(sent_df)\n",
        "tokenized.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsYyYpFZRyxY",
        "outputId": "8bf7fa37-6306-4b18-f021-89fcb5dac632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------------------------------------+----------------------------------------------------------------+\n",
            "|id |sentences                                              |tokenOutput                                                     |\n",
            "+---+-------------------------------------------------------+----------------------------------------------------------------+\n",
            "|0  |hello i am happy to be learning Apache Spark           |[hello, i, am, happy, to, be, learning, apache, spark]          |\n",
            "|1  |I enjoy learning about python and javascript progamming|[i, enjoy, learning, about, python, and, javascript, progamming]|\n",
            "|2  |i am familiar with Machine Learning applications       |[i, am, familiar, with, machine, learning, applications]        |\n",
            "|3  |here, is,a,list,of,words                               |[here,, is,a,list,of,words]                                     |\n",
            "+---+-------------------------------------------------------+----------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized.select('sentences','tokenOutput').withColumn(\"tokens\", countTokens(col('tokenOutput'))).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17RZsvZpR-st",
        "outputId": "9b9173c7-5dc6-4ec7-ffd5-e38e3feafdba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------+----------------------------------------------------------------+------+\n",
            "|sentences                                              |tokenOutput                                                     |tokens|\n",
            "+-------------------------------------------------------+----------------------------------------------------------------+------+\n",
            "|hello i am happy to be learning Apache Spark           |[hello, i, am, happy, to, be, learning, apache, spark]          |9     |\n",
            "|I enjoy learning about python and javascript progamming|[i, enjoy, learning, about, python, and, javascript, progamming]|8     |\n",
            "|i am familiar with Machine Learning applications       |[i, am, familiar, with, machine, learning, applications]        |7     |\n",
            "|here, is,a,list,of,words                               |[here,, is,a,list,of,words]                                     |2     |\n",
            "+-------------------------------------------------------+----------------------------------------------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regexTokenized = regexTokenizer.transform(sent_df)\n",
        "regexTokenized.select('sentences','regxOutput').withColumn(\"tokens\", countTokens(col('regxOutput'))).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQF0F_PRSpOe",
        "outputId": "9e24bad1-21c9-442f-f8c7-f73e1b11f7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------+----------------------------------------------------------------+------+\n",
            "|sentences                                              |regxOutput                                                      |tokens|\n",
            "+-------------------------------------------------------+----------------------------------------------------------------+------+\n",
            "|hello i am happy to be learning Apache Spark           |[hello, i, am, happy, to, be, learning, apache, spark]          |9     |\n",
            "|I enjoy learning about python and javascript progamming|[i, enjoy, learning, about, python, and, javascript, progamming]|8     |\n",
            "|i am familiar with Machine Learning applications       |[i, am, familiar, with, machine, learning, applications]        |7     |\n",
            "|here, is,a,list,of,words                               |[here, is, a, list, of, words]                                  |6     |\n",
            "+-------------------------------------------------------+----------------------------------------------------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stop Word Removal"
      ],
      "metadata": {
        "id": "HdWxABYhUxZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover"
      ],
      "metadata": {
        "id": "tSLdM_agUPOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remover = StopWordsRemover(inputCol='regxOutput',outputCol='cleaned')\n",
        "remover.transform(regexTokenized).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3Rs09ADU60y",
        "outputId": "588be6b0-dce7-47cd-c104-c5dc80a8ce5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------------------------------------+----------------------------------------------------------------+-------------------------------------------------+\n",
            "|id |sentences                                              |regxOutput                                                      |cleaned                                          |\n",
            "+---+-------------------------------------------------------+----------------------------------------------------------------+-------------------------------------------------+\n",
            "|0  |hello i am happy to be learning Apache Spark           |[hello, i, am, happy, to, be, learning, apache, spark]          |[hello, happy, learning, apache, spark]          |\n",
            "|1  |I enjoy learning about python and javascript progamming|[i, enjoy, learning, about, python, and, javascript, progamming]|[enjoy, learning, python, javascript, progamming]|\n",
            "|2  |i am familiar with Machine Learning applications       |[i, am, familiar, with, machine, learning, applications]        |[familiar, machine, learning, applications]      |\n",
            "|3  |here, is,a,list,of,words                               |[here, is, a, list, of, words]                                  |[list, words]                                    |\n",
            "+---+-------------------------------------------------------+----------------------------------------------------------------+-------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### n-gram"
      ],
      "metadata": {
        "id": "db9stWVUaHy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import NGram"
      ],
      "metadata": {
        "id": "51Uffv8jVSSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOdYRWDBaSpG",
        "outputId": "cbe3835f-ae4e-45a4-f62e-046c1f48165b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------------------------------------+----------------------------------------------------------------+\n",
            "|id |sentences                                              |tokenOutput                                                     |\n",
            "+---+-------------------------------------------------------+----------------------------------------------------------------+\n",
            "|0  |hello i am happy to be learning Apache Spark           |[hello, i, am, happy, to, be, learning, apache, spark]          |\n",
            "|1  |I enjoy learning about python and javascript progamming|[i, enjoy, learning, about, python, and, javascript, progamming]|\n",
            "|2  |i am familiar with Machine Learning applications       |[i, am, familiar, with, machine, learning, applications]        |\n",
            "|3  |here, is,a,list,of,words                               |[here,, is,a,list,of,words]                                     |\n",
            "+---+-------------------------------------------------------+----------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = NGram(n=2,inputCol='tokenOutput',outputCol='bigrams')\n",
        "bigram_df = bigram.transform(tokenized)\n",
        "bigram_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8dwSI_uaYyQ",
        "outputId": "d5548728-c10b-4372-e7e1-9d9ddfda6f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n",
            "|id |sentences                                              |tokenOutput                                                     |bigrams                                                                                                   |\n",
            "+---+-------------------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n",
            "|0  |hello i am happy to be learning Apache Spark           |[hello, i, am, happy, to, be, learning, apache, spark]          |[hello i, i am, am happy, happy to, to be, be learning, learning apache, apache spark]                    |\n",
            "|1  |I enjoy learning about python and javascript progamming|[i, enjoy, learning, about, python, and, javascript, progamming]|[i enjoy, enjoy learning, learning about, about python, python and, and javascript, javascript progamming]|\n",
            "|2  |i am familiar with Machine Learning applications       |[i, am, familiar, with, machine, learning, applications]        |[i am, am familiar, familiar with, with machine, machine learning, learning applications]                 |\n",
            "|3  |here, is,a,list,of,words                               |[here,, is,a,list,of,words]                                     |[here, is,a,list,of,words]                                                                                |\n",
            "+---+-------------------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Featue Extraction \n",
        "- TF-IDF"
      ],
      "metadata": {
        "id": "WqtKK8YtbTMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import IDF, HashingTF, Tokenizer"
      ],
      "metadata": {
        "id": "OyNrpRjwbC8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df = spark.createDataFrame(\n",
        "    [\n",
        "     (0, 0.0,\"hello i am happy to be learning Apache Spark\"),\n",
        "     (1, 0.0,\"I enjoy learning about python and javascript progamming\"),\n",
        "     (2, 1.0,\"i am familiar with Machine Learning applications\"),\n",
        "     (3, 1.0, \"here, is,a,list,of,words\")\n",
        "    ],\n",
        "    ['id','label','sentences']\n",
        ")\n",
        "\n",
        "sent_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8eD5oEnbjUb",
        "outputId": "e5179361-4f98-40ed-830a-e6ca3baa7c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------------------------------------------------------+\n",
            "|id |label|sentences                                              |\n",
            "+---+-----+-------------------------------------------------------+\n",
            "|0  |0.0  |hello i am happy to be learning Apache Spark           |\n",
            "|1  |0.0  |I enjoy learning about python and javascript progamming|\n",
            "|2  |1.0  |i am familiar with Machine Learning applications       |\n",
            "|3  |1.0  |here, is,a,list,of,words                               |\n",
            "+---+-----+-------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexTokenizer(inputCol='sentences',outputCol='words',pattern=\"\\\\W\")\n",
        "words_df  = tokenizer.transform(sent_df)\n",
        "words_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSfb5ui5fa9P",
        "outputId": "129aeda1-acb8-4f55-9145-5412bc375508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------------------------------------------------------+----------------------------------------------------------------+\n",
            "|id |label|sentences                                              |words                                                           |\n",
            "+---+-----+-------------------------------------------------------+----------------------------------------------------------------+\n",
            "|0  |0.0  |hello i am happy to be learning Apache Spark           |[hello, i, am, happy, to, be, learning, apache, spark]          |\n",
            "|1  |0.0  |I enjoy learning about python and javascript progamming|[i, enjoy, learning, about, python, and, javascript, progamming]|\n",
            "|2  |1.0  |i am familiar with Machine Learning applications       |[i, am, familiar, with, machine, learning, applications]        |\n",
            "|3  |1.0  |here, is,a,list,of,words                               |[here, is, a, list, of, words]                                  |\n",
            "+---+-----+-------------------------------------------------------+----------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hashingTF = HashingTF(inputCol='words',outputCol='rawFeatures', numFeatures=20)\n",
        "featurized = hashingTF.transform(words_df)"
      ],
      "metadata": {
        "id": "A4XvFFRhf_7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featurized.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyHwA81rgYhd",
        "outputId": "f149fb57-1ca0-4cba-bc00-e8682800179d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------------------------------------------------------+----------------------------------------------------------------+-----------------------------------------------------------------+\n",
            "|id |label|sentences                                              |words                                                           |rawFeatures                                                      |\n",
            "+---+-----+-------------------------------------------------------+----------------------------------------------------------------+-----------------------------------------------------------------+\n",
            "|0  |0.0  |hello i am happy to be learning Apache Spark           |[hello, i, am, happy, to, be, learning, apache, spark]          |(20,[3,5,6,7,8,9,12,15,16],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "|1  |0.0  |I enjoy learning about python and javascript progamming|[i, enjoy, learning, about, python, and, javascript, progamming]|(20,[5,6,9,11,12,14,16],[1.0,1.0,1.0,1.0,1.0,1.0,2.0])           |\n",
            "|2  |1.0  |i am familiar with Machine Learning applications       |[i, am, familiar, with, machine, learning, applications]        |(20,[0,2,5,10,12,16],[1.0,2.0,1.0,1.0,1.0,1.0])                  |\n",
            "|3  |1.0  |here, is,a,list,of,words                               |[here, is, a, list, of, words]                                  |(20,[7,8,9,12,15],[1.0,1.0,1.0,1.0,2.0])                         |\n",
            "+---+-----+-------------------------------------------------------+----------------------------------------------------------------+-----------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idf = IDF(inputCol='rawFeatures',outputCol='features')\n",
        "idf_model = idf.fit(featurized)"
      ],
      "metadata": {
        "id": "BjTGvvKmgeuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rescale = idf_model.transform(featurized)\n",
        "rescale.select(\"label\",'features').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jCfJgmBgxpg",
        "outputId": "536170d6-2b9c-4993-f684-8ac1f009f1f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|label|features                                                                                                                                                                                    |\n",
            "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0.0  |(20,[3,5,6,7,8,9,12,15,16],[0.9162907318741551,0.22314355131420976,0.5108256237659907,0.5108256237659907,0.5108256237659907,0.22314355131420976,0.0,0.5108256237659907,0.22314355131420976])|\n",
            "|0.0  |(20,[5,6,9,11,12,14,16],[0.22314355131420976,0.5108256237659907,0.22314355131420976,0.9162907318741551,0.0,0.9162907318741551,0.44628710262841953])                                         |\n",
            "|1.0  |(20,[0,2,5,10,12,16],[0.9162907318741551,1.8325814637483102,0.22314355131420976,0.9162907318741551,0.0,0.22314355131420976])                                                                |\n",
            "|1.0  |(20,[7,8,9,12,15],[0.5108256237659907,0.5108256237659907,0.22314355131420976,0.0,1.0216512475319814])                                                                                       |\n",
            "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Count Vectorization"
      ],
      "metadata": {
        "id": "GSxMxWyjhKdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer"
      ],
      "metadata": {
        "id": "3qNIloX5g8IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(\n",
        "    [\n",
        "     (0,list('abcde')),\n",
        "      (1,list('abbbccddee')),\n",
        "    ],\n",
        "    ['id','words']\n",
        ")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLiLEy5-hXYm",
        "outputId": "f0098708-363e-44c0-82a7-b8dd2d85852a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|               words|\n",
            "+---+--------------------+\n",
            "|  0|     [a, b, c, d, e]|\n",
            "|  1|[a, b, b, b, c, c...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(inputCol='words', outputCol='features', vocabSize=5,minDF=2.0)\n",
        "model = cv.fit(df)\n",
        "res = model.transform(df)\n",
        "res.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a27p9gLFhkYL",
        "outputId": "aa204273-0f50-45c0-b881-cde6ae16ca5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------+-------------------------------------+\n",
            "|id |words                         |features                             |\n",
            "+---+------------------------------+-------------------------------------+\n",
            "|0  |[a, b, c, d, e]               |(5,[0,1,2,3,4],[1.0,1.0,1.0,1.0,1.0])|\n",
            "|1  |[a, b, b, b, c, c, d, d, e, e]|(5,[0,1,2,3,4],[3.0,2.0,2.0,2.0,1.0])|\n",
            "+---+------------------------------+-------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NLP with Naive Bayse : PySpark"
      ],
      "metadata": {
        "id": "KS5egGaEfpDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head SMSSpamCollection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kXt4mANf8gk",
        "outputId": "c93c11bb-a4a4-42a6-8c85-1db0d455426e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "ham\tOk lar... Joking wif u oni...\n",
            "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "ham\tU dun say so early hor... U c already then say...\n",
            "ham\tNah I don't think he goes to usf, he lives around here though\n",
            "spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
            "ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
            "ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
            "spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
            "spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df  = spark.read.csv(\"/content/SMSSpamCollection\", inferSchema=True, sep='\\t')\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI-COmF1h3-t",
        "outputId": "28565a93-e829-40c7-aca0-8134ccd99b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+\n",
            "| _c0|                 _c1|\n",
            "+----+--------------------+\n",
            "| ham|Go until jurong p...|\n",
            "| ham|Ok lar... Joking ...|\n",
            "|spam|Free entry in 2 a...|\n",
            "| ham|U dun say so earl...|\n",
            "| ham|Nah I don't think...|\n",
            "+----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumnRenamed('_c0',\"class\").withColumnRenamed(\"_c1\", \"text\")\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCr8y0IufKLd",
        "outputId": "8fc910c6-4f6d-4ad0-d576-86aa0d253ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|class|                text|\n",
            "+-----+--------------------+\n",
            "|  ham|Go until jurong p...|\n",
            "|  ham|Ok lar... Joking ...|\n",
            "| spam|Free entry in 2 a...|\n",
            "|  ham|U dun say so earl...|\n",
            "|  ham|Nah I don't think...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clean Data"
      ],
      "metadata": {
        "id": "W77o-hz6gdvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import length"
      ],
      "metadata": {
        "id": "fxJCT3ScfKWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"length\", length(df['text']))\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut1idSLkfKY-",
        "outputId": "0fc9b8e9-1fe2-4905-eb86-c7c3bd529476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+------+\n",
            "|class|                text|length|\n",
            "+-----+--------------------+------+\n",
            "|  ham|Go until jurong p...|   111|\n",
            "|  ham|Ok lar... Joking ...|    29|\n",
            "| spam|Free entry in 2 a...|   155|\n",
            "|  ham|U dun say so earl...|    49|\n",
            "|  ham|Nah I don't think...|    61|\n",
            "+-----+--------------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('class').mean().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5onCWFjPfKbt",
        "outputId": "0d9e86eb-cbfb-40e8-8619-5c15cb9d8f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|class|      avg(length)|\n",
            "+-----+-----------------+\n",
            "|  ham|71.45431945307645|\n",
            "| spam|138.6706827309237|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Transformation"
      ],
      "metadata": {
        "id": "NAhofX0Dg_i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, StringIndexer"
      ],
      "metadata": {
        "id": "GVLSSEYafKe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(inputCol='text',outputCol='token_text')\n",
        "stop_word_remover = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
        "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
        "idf = IDF(inputCol=\"c_vec\",outputCol='tf_idf')\n",
        "ham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')"
      ],
      "metadata": {
        "id": "kXAZ-L8JhIHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vector"
      ],
      "metadata": {
        "id": "mIBJZls1jATc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned = VectorAssembler(inputCols=['tf_idf','length'], outputCol='features')"
      ],
      "metadata": {
        "id": "9CmhI_hbjANp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### loading model"
      ],
      "metadata": {
        "id": "J9-88Qtpjc0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes"
      ],
      "metadata": {
        "id": "sVAQ9NzqjADj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb = NaiveBayes()"
      ],
      "metadata": {
        "id": "gf_rSkQMjzx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline\n",
        "from pyspark.ml import Pipeline"
      ],
      "metadata": {
        "id": "F7PqbD8nj1eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[\n",
        "                            ham_spam_to_num,\n",
        "                            tokenizer,\n",
        "                            stop_word_remover,\n",
        "                            count_vec,\n",
        "                            idf,\n",
        "                            cleaned])"
      ],
      "metadata": {
        "id": "pccSXW9-j4Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaner = pipeline.fit(df)"
      ],
      "metadata": {
        "id": "t5Noj_Q4kN4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = cleaner.transform(df)"
      ],
      "metadata": {
        "id": "l6M-RjtmkUCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itf6vrnbkkVR",
        "outputId": "5edc6060-625c-4a0f-eea7-1ed7505ca44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|class|                text|length|label|          token_text|         stop_tokens|               c_vec|              tf_idf|            features|\n",
            "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  ham|Go until jurong p...|   111|  0.0|[go, until, juron...|[go, jurong, poin...|(13423,[7,11,31,6...|(13423,[7,11,31,6...|(13424,[7,11,31,6...|\n",
            "|  ham|Ok lar... Joking ...|    29|  0.0|[ok, lar..., joki...|[ok, lar..., joki...|(13423,[0,24,297,...|(13423,[0,24,297,...|(13424,[0,24,297,...|\n",
            "| spam|Free entry in 2 a...|   155|  1.0|[free, entry, in,...|[free, entry, 2, ...|(13423,[2,13,19,3...|(13423,[2,13,19,3...|(13424,[2,13,19,3...|\n",
            "|  ham|U dun say so earl...|    49|  0.0|[u, dun, say, so,...|[u, dun, say, ear...|(13423,[0,70,80,1...|(13423,[0,70,80,1...|(13424,[0,70,80,1...|\n",
            "|  ham|Nah I don't think...|    61|  0.0|[nah, i, don't, t...|[nah, think, goes...|(13423,[36,134,31...|(13423,[36,134,31...|(13424,[36,134,31...|\n",
            "+-----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training and evaluation"
      ],
      "metadata": {
        "id": "C0WoLObckeCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = clean_df.select(['label','features'])\n",
        "clean_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwLFiwXqkWFf",
        "outputId": "4a09b93c-f042-4070-9052-15b8bcf804a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(13424,[7,11,31,6...|\n",
            "|  0.0|(13424,[0,24,297,...|\n",
            "|  1.0|(13424,[2,13,19,3...|\n",
            "|  0.0|(13424,[0,70,80,1...|\n",
            "|  0.0|(13424,[36,134,31...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train, test) = clean_df.randomSplit([0.7,0.3],seed=42)"
      ],
      "metadata": {
        "id": "-ED5EzLXkvh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = nb.fit(train)"
      ],
      "metadata": {
        "id": "uDLbrOnsk_lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = prediction.transform(test)"
      ],
      "metadata": {
        "id": "DjibjHYalBsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7oKXLuPlHTq",
        "outputId": "45ccdb53-185d-4c41-e522-8ba2a4e76668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|label|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|  0.0|(13424,[0,1,2,41,...|[-1060.7325420854...|[1.0,9.6391158107...|       0.0|\n",
            "|  0.0|(13424,[0,1,5,20,...|[-803.13623340156...|[1.0,2.7071860143...|       0.0|\n",
            "|  0.0|(13424,[0,1,7,8,1...|[-1152.0926413349...|[1.0,6.3682506790...|       0.0|\n",
            "|  0.0|(13424,[0,1,7,15,...|[-656.71821333935...|[1.0,7.6641099247...|       0.0|\n",
            "|  0.0|(13424,[0,1,12,33...|[-444.22584589378...|[1.0,1.4534997554...|       0.0|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "metadata": {
        "id": "oj_yjYI3lIWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval = MulticlassClassificationEvaluator()\n",
        "acc = eval.evaluate(res)\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ITyFp8JlQPU",
        "outputId": "5d1b636c-6182-4a9f-bb45-e05c08c30ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9266021977210805"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vU4I3vzllWSi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}